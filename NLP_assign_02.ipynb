{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What are Corpora?"
      ],
      "metadata": {
        "id": "SoJ8kyT5NBFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Natural Language Processing (NLP), a corpus refers to a large collection of electronic text that is used to train and test algorithms for various NLP tasks, such as machine translation, text classification, named entity recognition, and sentiment analysis.These corpora can be created manually or automatically, and can be composed of various types of text, including news articles, books, emails, social media posts, and transcripts of spoken language."
      ],
      "metadata": {
        "id": "Gcf91bVENXUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are Tokens?"
      ],
      "metadata": {
        "id": "6c1hab7CNu2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A token is a word, number, punctuation mark, or any other entity that represents a meaningful unit of language in a given context.Tokenization is the process of breaking down a piece of text into individual tokens. The tokens are usually separated by whitespace or punctuation, and the resulting sequence of tokens is called a tokenized text. \n",
        "\n",
        "Example:\n",
        "\n",
        "\"John likes to play football.\"\n",
        "\n",
        "The tokenized form of this sentence would be:\n",
        "\n",
        "[\"John\", \"likes\", \"to\", \"play\", \"football\", \".\"]"
      ],
      "metadata": {
        "id": "hjIZTL8mN1IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are Unigrams, Bigrams, Trigrams?"
      ],
      "metadata": {
        "id": "9Ry0OhBkOWeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A unigram is a single token or word in a given text. For example, in the sentence \"The quick brown fox jumps over the lazy dog\", the unigrams would be \"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", and \"dog\".\n",
        "\n",
        "A bigram is a sequence of two consecutive tokens in a given text. For example, in the same sentence \"The quick brown fox jumps over the lazy dog\", the bigrams would be \"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", \"the lazy\", and \"lazy dog\".\n",
        "\n",
        "A trigram is a sequence of three consecutive tokens in a given text. For example, in the same sentence \"The quick brown fox jumps over the lazy dog\", the trigrams would be \"The quick brown\", \"quick brown fox\", \"brown fox jumps\", \"fox jumps over\", \"jumps over the\", \"over the lazy\", and \"the lazy dog\"."
      ],
      "metadata": {
        "id": "23O8YCJJOgNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How to generate n-grams from text?"
      ],
      "metadata": {
        "id": "WZfx6nu4PIHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can generate n-grams from text using the nltk (Natural Language Toolkit) library.This code first converts the text to lowercase and tokenizes it using the nltk.word_tokenize function. Then, it generates unigrams, bigrams, and trigrams using the ngrams function from nltk. Finally, it prints the resulting n-grams for each sequence length."
      ],
      "metadata": {
        "id": "SQZa2chfPPYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# from nltk import ngrams\n",
        "\n",
        "# # define the text\n",
        "# text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# # convert the text to lowercase and tokenize it\n",
        "# tokens =nltk.word_tokenize(text.lower())\n",
        "\n",
        "# # generate unigrams\n",
        "# unigrams = list(ngrams(tokens, 1))\n",
        "\n",
        "# # generate bigrams\n",
        "# bigrams = list(ngrams(tokens, 2))\n",
        "\n",
        "# # generate trigrams\n",
        "# trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "# # print the n-grams\n",
        "# print(\"Unigrams:\", unigrams)\n",
        "# print(\"Bigrams:\", bigrams)\n",
        "# print(\"Trigrams:\", trigrams)"
      ],
      "metadata": {
        "id": "u-9KKfw8VoQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain Lemmatization"
      ],
      "metadata": {
        "id": "4E63OiGKeNmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is a text normalization technique used in Natural Language Processing (NLP) that involves reducing words to their base form. The lemma is the root form of the word, and lemmatization maps different inflected forms of a word to a single base form, which can reduce the dimensionality of the text and facilitate further analysis.\n",
        "\n",
        "\"am\", \"are\", \"is\" -> \"be\"\n",
        "\n",
        "\"car\", \"cars\", \"car's\", \"cars'\" -> \"car\"\n",
        "\n",
        "\"running\", \"run\", \"ran\" -> \"run\""
      ],
      "metadata": {
        "id": "X0FHydZsemIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain Stemming"
      ],
      "metadata": {
        "id": "pBEn61BXgD46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is the process of reducing a word to its base or root form, often by removing suffixes or prefixes. The resulting base or root form is known as the stem.For example, the words \"run\", \"running,\" and \"runner\" all have the same stem \"run,\" which can help in tasks such as text classification, information retrieval, and natural language processing. Some commonly used stemming algorithms include the Porter stemming algorithm and the Snowball stemming algorithm."
      ],
      "metadata": {
        "id": "Vum9iT_FYieW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Explain Part-of-speech (POS) tagging"
      ],
      "metadata": {
        "id": "ycCNubqjYM56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-speech (POS) tagging is the process of analyzing a sentence and labeling each word with its corresponding part of speech, such as noun, verb, adjective, adverb, pronoun, preposition, conjunction, or interjection.The purpose of POS tagging is to provide a more detailed understanding of the grammatical structure and meaning of a sentence. It can help in various natural language processing tasks, such as text classification, sentiment analysis, machine translation, and information retrieval.For example, in the sentence \"The cat sat on the mat,\" a POS tagger would label \"the\" as a determiner, \"cat\" as a noun, \"sat\" as a verb, \"on\" as a preposition, \"the\" again as a determiner, and \"mat\" as a noun."
      ],
      "metadata": {
        "id": "ytdjFSfgZTw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Explain Chunking or shallow parsing"
      ],
      "metadata": {
        "id": "bP712EFhYPAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking, also known as shallow parsing, is a natural language processing technique that involves identifying and grouping together related words in a sentence based on their grammatical structure. The resulting groups of words are called chunks, and they often correspond to meaningful units such as noun phrases, verb phrases, or prepositional phrases.The purpose of chunking is to extract higher-level information from a sentence that can be used in various natural language processing tasks, such as information extraction, question answering, and text summarization.Chunking can be seen as a middle ground between POS tagging and full syntactic parsing. While POS tagging only labels individual words with their parts of speech, and full syntactic parsing analyzes the entire sentence to build a tree-like structure of its grammatical relationships, chunking focuses on identifying and extracting meaningful chunks from a sentence without fully parsing it."
      ],
      "metadata": {
        "id": "G5BXiFQlaPyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain Noun Phrase (NP) chunking"
      ],
      "metadata": {
        "id": "lpb14sd5YTlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noun Phrase (NP) chunking is a specific type of chunking or shallow parsing that focuses on identifying and extracting noun phrases from a sentence. A noun phrase is a group of words that act together to function as a noun in a sentence, such as \"the cat\" or \"a red apple.\"The process of NP chunking involves using part-of-speech tagging (POS tagging) to identify the nouns, pronouns, adjectives, and other parts of speech in a sentence. From there, the algorithm searches for patterns in the sentence structure that indicate the presence of an NP, such as a determiner followed by an adjective and a noun. Once the NP has been identified, it can be extracted and analyzed further."
      ],
      "metadata": {
        "id": "_KbIR-pfbdL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Explain Named Entity Recognition"
      ],
      "metadata": {
        "id": "KtdzlJXyYWok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER) is a natural language processing technique that involves identifying and classifying entities within a text. An entity can refer to any type of named object or concept, such as people, organizations, locations, products, and events.Once the algorithm has been trained, it can be used to analyze new text and automatically identify and classify entities. This can involve a variety of techniques, such as rule-based methods, statistical models, and deep learning approaches."
      ],
      "metadata": {
        "id": "ETtKFdntinSk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTdm5seVkO87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}