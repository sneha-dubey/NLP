{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the basic architecture of RNN cell."
      ],
      "metadata": {
        "id": "DnP2DR9AGNqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic architecture of an RNN cell consists of three components:\n",
        "\n",
        "a.) Input: At each time step, the RNN cell receives an input vector x(t) as the input. This input vector can represent a single element of the sequence or a window of elements.\n",
        "\n",
        "b.) Hidden State: The RNN cell also maintains a hidden state vector h(t), which is updated at each time step based on the current input and the previous hidden state. The hidden state vector can be thought of as a memory of the previous inputs and can capture information from the sequence history.\n",
        "\n",
        "c.) Output: The RNN cell produces an output vector y(t) at each time step, which is computed based on the current input and the current hidden state. The output vector can be used to make predictions or generate new sequences.\n",
        "\n",
        "output of hidden state : h(t) = f(Wxhx(t) + Whhh(t-1) + b)\n",
        "The output at each time step can be computed as: y(t) = g(Why*h(t) + b')"
      ],
      "metadata": {
        "id": "zC-O6RwTHGGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain Backpropagation through time (BPTT)"
      ],
      "metadata": {
        "id": "iINFyWMMGV_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation through time (BPTT) is a variant of the backpropagation algorithm that is used to train recurrent neural networks (RNNs) for processing sequential data. BPTT extends the standard backpropagation algorithm to handle the temporal dependencies that exist in sequential data by propagating the error gradients through time.The BPTT algorithm works by unrolling the recurrent neural network for a fixed number of time steps and treating it as a feedforward neural network. \n",
        "\n",
        "BPTT algorithm works as follows:\n",
        "\n",
        "a.)forward pass: The input sequence x(1), x(2), ..., x(T) is fed into the recurrent neural network, which produces an output sequence y(1), y(2), ..., y(T).\n",
        "\n",
        "b.)Error computation: The error between the predicted output and the actual output is computed as the sum of the error at each time step:\n",
        "\n",
        "E = sum(y(t) - y_true(t))^2\n",
        "\n",
        "c.)Backpropagation through time: The error gradient with respect to the output at each time step is computed using the chain rule:\n",
        "\n",
        "delta_y(t) = 2 * (y(t) - y_true(t))\n",
        "\n",
        "d.)Weight update: The error gradients with respect to the weights are computed using the error gradients with respect to the hidden state and input:\n",
        "\n",
        "dWxh = delta_h(t) * x(t)\n",
        "dWhh = delta_h(t) * h(t-1)\n",
        "dWhy = delta_y(t) * h(t)\n",
        "\n",
        "e.)Repeat: Steps 1-4 are repeated for a fixed number of epochs or until the error converges."
      ],
      "metadata": {
        "id": "tz3tnhqTHbIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain Vanishing and exploding gradients"
      ],
      "metadata": {
        "id": "mLCYWr2uGaOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanishing and exploding gradients are common problems that can occur during the training of neural networks, particularly deep neural networks with many layers. These problems arise due to the way that gradients are propagated through the layers during the backpropagation algorithm.\n",
        "\n",
        "Vanishing gradients occur when the gradients become very small as they are propagated backwards through the layers of the network. This can make it difficult to update the weights of earlier layers in the network because the gradients are too small to cause any significant change. \n",
        "\n",
        "Exploding gradients occur when the gradients become very large as they are propagated backwards through the layers of the network. This can cause the weights to update too aggressively and can lead to numerical instability and divergence during training."
      ],
      "metadata": {
        "id": "P6PARBc7Jgvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain Long short-term memory (LSTM)"
      ],
      "metadata": {
        "id": "VlWiTcDeGiDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that is designed to handle the vanishing gradient problem and effectively model long-term dependencies in sequential data.\n",
        "\n",
        "The key idea behind LSTM is to introduce a memory cell that can selectively store or forget information at each time step. The memory cell is controlled by three gating mechanisms: the input gate, the output gate, and the forget gate. These gates are responsible for regulating the flow of information into and out of the memory cell."
      ],
      "metadata": {
        "id": "zGDG3_kmKLU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain Gated recurrent unit (GRU)"
      ],
      "metadata": {
        "id": "HA82Cw3LGlB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GRU is composed of two gates: an update gate and a reset gate. These gates are used to control the flow of information through the network and allow the network to selectively store or discard information from previous time steps.\n",
        "\n",
        "At each time step, the GRU takes as input the current input x(t), the previous hidden state h(t-1), and computes an intermediate hidden state h_hat(t). This intermediate state is then used to update the current hidden state h(t):\n",
        "\n",
        "a.)Update gate\n",
        "\n",
        "b.)reset gate\n",
        "\n",
        "c.)Candidate activation\n",
        "\n",
        "d.)Hidden state update"
      ],
      "metadata": {
        "id": "dhNeWBx1Kr6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain Peephole LSTM"
      ],
      "metadata": {
        "id": "4j2myUFuGoIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peephole LSTM is a variant of the Long Short-Term Memory (LSTM) network that extends the basic LSTM architecture by adding peephole connections that allow the gates to access the memory cell state directly.\n",
        "\n",
        "In a standard LSTM, the forget gate, input gate, and output gate are computed solely based on the input and previous hidden state, but in peephole LSTM, the memory cell state is also included in the computation of the gates."
      ],
      "metadata": {
        "id": "PRSrCf1LLgoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Bidirectional RNNs"
      ],
      "metadata": {
        "id": "3GyEkQggGrH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bidirectional RNNs (BRNNs) are a type of recurrent neural network architecture that process sequences in both forward and backward directions. They are used to capture information from both past and future context in a sequence, and have shown great success in natural language processing, speech recognition, and other sequence modeling tasks.\n",
        "\n",
        "In a BRNN, the input sequence is fed into two separate RNNs: one in the forward direction and the other in the backward direction. The outputs of both RNNs are then concatenated to produce the final output sequence. At each time step, the forward RNN processes the input sequence from the beginning to the current time step, while the backward RNN processes the input sequence from the end to the current time step."
      ],
      "metadata": {
        "id": "cQc4vpeCL3Af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Explain the gates of LSTM with equations."
      ],
      "metadata": {
        "id": "olMpUhXyGtxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM (Long Short-Term Memory) networks are a type of recurrent neural network (RNN) architecture that are capable of capturing long-term dependencies in sequential data.The key to their success lies in their ability to selectively store, update, and retrieve information from the memory cell state, which is controlled by three types of gates: the forget gate, the input gate, and the output gate.\n",
        "\n",
        "a.) Forget gate: f(t) = σ(Wf[h(t-1), x(t)] + bf)\n",
        "\n",
        "where σ is the sigmoid activation function, Wf is the weight matrix, and bf is the bias vector.\n",
        "\n",
        "b.)Input gate: The input gate is defined by the following equation:\n",
        "\n",
        "i(t) = σ(Wi[h(t-1), x(t)] + bi)\n",
        "\n",
        "where σ is the sigmoid activation function, Wi is the weight matrix, and bi is the bias vector.\n",
        "\n",
        "c.)Candidate cell state:The candidate cell state is defined by the following equation:\n",
        "ĉ(t) = tanh(Wc[h(t-1), x(t)] + bc)\n",
        "where tanh is the hyperbolic tangent activation function, Wc is the weight matrix, and bc is the bias vector.\n",
        "\n",
        "d.)Memory cell sate update: The new memory cell state (c(t)) is computed as follows:\n",
        "c(t) = f(t) * c(t-1) + i(t) * ĉ(t)\n",
        "\n",
        "e.)Output gate:The output gate is defined by the following equation:\n",
        "\n",
        "o(t) = σ(Wo[h(t-1), x(t)] + bo)\n",
        "\n",
        "where σ is the sigmoid activation function, Wo is the weight matrix, and bo is the bias vector.\n",
        "\n",
        "f.)Hidden state update:The hidden state is computed as a nonlinear function of the memory cell state and the output gate. The output at the current time step (y(t)) is computed as follows:\n",
        "\n",
        "h(t) = o(t) * tanh(c(t))\n",
        "\n",
        "where tanh is the hyperbolic tangent activation function."
      ],
      "metadata": {
        "id": "ZxmXyRzoMSOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain BiLSTM"
      ],
      "metadata": {
        "id": "fsD5Asf6GwZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bidirectional LSTM (BiLSTM) is a type of neural network that combines two LSTM networks, one processing the input sequence in a forward direction and the other processing it in a backward direction. This allows the network to capture both past and future context of each input in a sequence, making it particularly useful for tasks that require understanding of the entire sequence, such as natural language processing and speech recognition.\n",
        "\n",
        "Mathematically, the forward LSTM can be defined as follows:\n",
        "\n",
        "Input gate: i_t = sigmoid(W_ix_t + U_ih_{t-1} + b_i)\n",
        "Forget gate: f_t = sigmoid(W_fx_t + U_fh_{t-1} + b_f)\n",
        "Candidate memory cell: \\tilde{C_t} = tanh(W_cx_t + U_ch_{t-1} + b_c)\n",
        "Memory cell: C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C_t}\n",
        "Output gate: o_t = sigmoid(W_ox_t + U_oh_{t-1} + b_o)\n",
        "Hidden state: h_t = o_t \\odot tanh(C_t)\n",
        "where x_t is the input at time step t, h_t is the hidden state at time step t, C_t is the memory cell state at time step t, and \\odot represents element-wise multiplication.\n",
        "\n",
        "Similarly, the backward LSTM can be defined as follows:\n",
        "\n",
        "Input gate: i_t' = sigmoid(W_i'x_t' + U_i'h_{t+1}' + b_i')\n",
        "Forget gate: f_t' = sigmoid(W_f'x_t' + U_f'h_{t+1}' + b_f')\n",
        "Candidate memory cell: \\tilde{C_t'} = tanh(W_c'x_t' + U_c'h_{t+1}' + b_c')\n",
        "Memory cell: C_t' = f_t' \\odot C_{t+1}' + i_t' \\odot \\tilde{C_t'}\n",
        "Output gate: o_t' = sigmoid(W_o'x_t' + U_o'h_{t+1}' + b_o')\n",
        "Hidden state: h_t' = o_t' \\odot tanh(C_t')\n",
        "where x_t' is the input at time step t in the reverse direction, h_t' is the hidden state at time step t in the reverse direction, and C_t' is the memory cell state at time step t in the reverse direction.\n",
        "\n",
        "The final hidden state for each time step is the concatenation of the hidden states from the forward and backward LSTMs:\n",
        "\n",
        "h_t^{BiLSTM} = [h_t,"
      ],
      "metadata": {
        "id": "eSCIuQnTPNjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Explain BiGRU"
      ],
      "metadata": {
        "id": "KA4h5YKGGzUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bidirectional Gated Recurrent Unit (BiGRU) is a type of neural network that is similar to Bidirectional LSTM, but uses GRU cells instead of LSTM cells. It is designed to capture both past and future context of each input in a sequence, making it useful for tasks that require understanding of the entire sequence, such as natural language processing and speech recognition.Like BiLSTM, BiGRU consists of two GRU networks, one processing the input sequence in a forward direction and the other processing it in a backward direction. At each time step, the hidden state of the forward GRU is updated based on the input at that time step and the hidden state from the previous time step. Similarly, the hidden state of the backward GRU is updated based on the input at that time step and the hidden state from the next time step. The final hidden state at each time step is the concatenation of the hidden states from the forward and backward GRUs, which captures both past and future context of the input at that time step."
      ],
      "metadata": {
        "id": "D_r4fomRPypM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ycZU_90XQcDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}