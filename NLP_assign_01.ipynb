{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain One-Hot Encoding"
      ],
      "metadata": {
        "id": "kQpBnO0ex3OB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is a technique used to represent categorical data as numerical data. Each category is represented as a binary vector with a length equal to the number of unique categories. The vector is all zeros except for a one in the position corresponding to the category.For example, we have a categorical feature \"fruit\" with three categories: apple, banana, and orange. We can represent each category using one-hot encoding as follows:\n",
        "\n",
        "apple: [1, 0, 0]\n",
        "\n",
        "banana: [0, 1, 0]\n",
        "\n",
        "orange: [0, 0, 1]"
      ],
      "metadata": {
        "id": "-47GoVZmx9Zv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain Bag of Words"
      ],
      "metadata": {
        "id": "gp4aa2iRyz4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bag of Words approach involves the following steps:\n",
        "\n",
        "Tokenization: Breaking the text into individual words, or tokens.\n",
        "\n",
        "Counting: Counting the frequency of each word in the text.\n",
        "\n",
        "Vectorization: Representing the text as a vector of word frequencies.\n",
        "\n",
        "Example:\n",
        "\n",
        "Document 1: \"The cat in the hat\"\n",
        "\n",
        "Document 2: \"The dog chased the cat\"\n",
        "\n",
        "We can represent these documents using Bag of Words as follows:\n",
        "\n",
        "            the\tcat\t in\t  hat\t dog\tchased\n",
        "Document 1\t 1\t  1\t  1\t   1\t 0\t    0\n",
        "\n",
        "Document 2\t 1\t  1\t  0\t   0\t 1\t    1\n"
      ],
      "metadata": {
        "id": "a1GQX1xcy3j_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain Bag of N-Grams"
      ],
      "metadata": {
        "id": "3rwhJL_G0vR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of N-Grams is a technique used to represent text data as numerical data in natural language processing (NLP). Similar to the Bag of Words technique, Bag of N-Grams represents a document as a bag, or multiset, of its words. However, instead of considering only individual words, Bag of N-Grams considers groups of N contiguous words, or N-grams.\n",
        "\n",
        "The Bag of N-Grams approach involves the following steps:\n",
        "\n",
        "a.)Tokenization: Breaking the text into individual words, or tokens.\n",
        "\n",
        "b.)N-gram extraction: Creating all possible sequences of N contiguous tokens from the text.\n",
        "\n",
        "c.)Counting: Counting the frequency of each N-gram in the text.\n",
        "\n",
        "d.)Vectorization: Representing the text as a vector of N-gram frequencies."
      ],
      "metadata": {
        "id": "rLZBmfgA1gHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain TF-IDF\n"
      ],
      "metadata": {
        "id": "9AEJ4-W81UtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF stands for Term Frequency-Inverse Document Frequency. The term frequency (TF) is a measure of how frequently a term appears in a particular document. The inverse document frequency (IDF) is a measure of how important a term is across the entire corpus, by calculating the logarithm of the ratio of the total number of documents to the number of documents containing the term.\n",
        "The formula for calculating TF-IDF score for a term t in a document d is:\n",
        "\n",
        "TF-IDF(t, d) = TF(t, d) x IDF(t)\n",
        "\n",
        "Where:\n",
        "TF(t, d) = frequency of term t in document d\n",
        "\n",
        "IDF(t) = logarithm of the ratio of the total number of documents in the corpus to the number of documents containing the term t\n",
        "\n",
        "By calculating the TF-IDF score, words that appear frequently in a document but also frequently in the corpus (such as stop words) will receive a low score. Conversely, words that are rare in the corpus but appear frequently in a document will receive a high score, indicating their importance in the document."
      ],
      "metadata": {
        "id": "km2DErnx24vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is OOV problem?"
      ],
      "metadata": {
        "id": "6azq6Uv44T6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OOV (Out-Of-Vocabulary) problem refers to the situation where a word or phrase is encountered in a text corpus or a natural language processing task that is not present in the vocabulary or dictionary being used. This can occur due to various reasons, such as misspellings, slang, abbreviations, acronyms, or words from different languages or domains."
      ],
      "metadata": {
        "id": "u2Nz7N1r5epm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What are word embeddings?"
      ],
      "metadata": {
        "id": "dTbkGTeZ57_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word embeddings are a type of representation of words in natural language processing (NLP) that allow words to be mapped from a high-dimensional space to a lower-dimensional space, while preserving their semantic relationships. Word embeddings are typically learned through training on large amounts of text data using techniques such as Word2Vec, GloVe, and FastText. Once trained, word embeddings can be used as input features for various NLP tasks such as sentiment analysis, named entity recognition, and machine translation. They have become a popular and effective tool for NLP because they can improve the accuracy of these tasks by capturing subtle semantic relationships between words that traditional methods such as bag-of-words cannot."
      ],
      "metadata": {
        "id": "xyeZJitm6cgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Explain Continuous bag of words (CBOW)"
      ],
      "metadata": {
        "id": "FJgfivG28gp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous bag of words (CBOW) is a popular neural network architecture used for natural language processing (NLP) tasks such as language modeling, sentiment analysis, and named entity recognition. CBOW is a type of neural network model that takes a sequence of words as input and predicts the probability of each word in the sequence based on the context of the other words in the sequence. Unlike other language models that predict words based on their position in the sentence, CBOW predicts words based on their surrounding context."
      ],
      "metadata": {
        "id": "rs_6n03J8pHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Explain SkipGram"
      ],
      "metadata": {
        "id": "PmjiXryT-gxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SkipGram is the opposite of Continuous bag of words (CBOW) architecture. In the SkipGram architecture, the objective is to predict the context words given a target word, rather than predicting the target word given the context words."
      ],
      "metadata": {
        "id": "wevuZAQb-hZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain Glove Embeddings."
      ],
      "metadata": {
        "id": "9zutfwhc-xzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe (Global Vectors for Word Representation) is a word embedding model that was introduced in 2014 by Stanford researchers. It is a unsupervised learning algorithm that creates dense vector representations (embeddings) for words in a corpus.The main idea behind GloVe is to capture the co-occurrence statistics of words in a corpus, which means how frequently two words appear together in a given context. The intuition behind this is that words that appear together frequently are likely to have similar meanings or semantic relationships.loVe uses a weighted least-squares regression model to learn word embeddings from the co-occurrence statistics. The model is trained on a global word-word co-occurrence matrix, where each element in the matrix represents the number of times a word appears in the context of another word."
      ],
      "metadata": {
        "id": "DEAzHmeS-5di"
      }
    }
  ]
}